This is the notes file for cugrid


TODO:
	- make slides to describe the current situation
		- description of grid and tensor setup -> done
		- screenshot of nsight -> done
		- benchmark times -> done
	- then proceed to rewrite the kernel calls
		- read about and test the grid structure -> not gonna do that
		- if that does not work ... rewrite to for loop over virtual nodes inside of kernel -> DONE!
	- finish the presentation
		- benchmarks on A100
	- rework random number filling
		- work on CPU




THOUGHTS:
    How are we gonna test the matmul for correctness? Maybe assist by numpy, or use some mathematical property?
    -> Fill vector with 1s and the rows of the matrices with numbers that we now the sum of and then check if the correct value of the sum is in the resulting vector
    -> Worked very nicely. I am convinced that malmul produces correct results

    Now we need to benchmark the matrix vector multiplication. But how?
    -> Just repeated matmul on several lattice sizes?
    -> Also want to test the effect of lenLane, even though this is not so important. Especially bc atm it needs to be set to the number of threads in one warp.

    Okay, the code seems to work. But how do I calculate the bandwidth and the flops? I have a grid of 8*16*16*32 with 64 dimensions (i.e. matrices: 64x64 and vector: 64) (double prec.) and 50 matmul operations took 2.405 sec. 
    So. How much bandwidth did we employ? To my understanding one matrix - vector multiplication takes N^2 + N memory loads. I.e. a total of (N^2 + N)*sizeof(double) bytes. But do I need to also take into account the write operation into the resulting vector? I do not think so. Okay, so ... with our parameters thats a total of sizeof(double)*(N^2 + 2*N)*8*16*16*32.
    The result is not very nice: something around 50 Gb/s ... meh.
    What about the flops? And how would it perform on an A100? Lets do flops first. So ... we have N^2 additions and multiplications. That makes a total of 2*N^2 flops per lattice site.
    Flops are bad ... like really bad. Out of the 20.31 TFlops, I get 500 MFlops. For 4-byte float that is.

    Okay, now we want to run this on juwels-booster. To this end we need to ssh into one of the computers of the physics CIP pool and set it up there.
    -> I made it into juwels-booster! The trick is to specify the key, that one wants to use, i.e., ssh sizmann1@juwels-booster.fz-juelich.de -i juwelsbooster.
    The next step is to build a source script and get an interactive session for one A100.
    -> got the session with $salloc --account=gm2dwf --gres=gpu:1 --nodes=1 --time=01:00:00 
        and ran the benchmark with srun ./bench_matmul
    -> Managed to do that :) But results were horrific:
        DURATION: 3586898
        BANDWIDTH: 30169 MBytes/sec
        ARITHMETICS: 299.351 Mflops

    I need to use diagnostics to find out what is going on. What about nsight? Let's find out!
	-> got it to work. The executable is /opt/nvidia/nsight-compute/2022.4.1/ncu-ui and needs to be executed with root privileges. The autoprofiling option together with the jump-to-next-kernel-call-button provides details about the kernel call. The program warns that the kernel calls are too small. 
	My suspicion about the kernel call overhead might have been right. In order to check that, we need to rewrite the code a little bit. I am thinking about employing the grid structure of cuda here, since it would require the least amount of rewriting. Without using this, I would need to substanially rethink on how to do the element indexing by thread identifier.

	Here is an important finding: Making the grid super small (2.2.2.4) and the matrix super large (N = 1024) makes the kernel very efficient in terms of memory bandwidth (also blocksize = 256). This means that indeed the number of kernel calls is problematic. Interestingly on the 3070mobile the performance descreases with N = 2048 and larger.

	So ... in order to rewrite everything to a singular kernel call we need to remap the looping domains to the threads. We have blockIdx, threadIdx and subsequentially laneIdx in terms of threads. Also we have latticeIdx, tensorIdcs and laneIdx. TensorIdcs become warp indices etc ... but how? Lets assume a given blocksize(bs) and therefore a given number of lanes per block (#lpb)
	We have x -> numVNodes, i(,j) -> N, l -> lenLane. This means we have numVNodes*N "lane calls", ergo numVNodes*N/#lpb blocks. What do we do if the division does not work out? -> add access guards! The kernel call becomes:
		blocks : (numVNodes*N + #lpb - 1)/#lpb
		threads: bs
		ker_matmul<<<blocks, threads>>>(res, mfield, vfield);
	The matmul function in iMatrix needs to change at well, or maybe we pull all the code into the Lattice class. Lets assume we do the second: 
		Then we first calculate the global warpIdx by warpIdxGlobal = blockIdx.x * #lpb + warpIdx. 
		From there we can calculate the vNodeIdx(i.e. x) x = warpIdxGlobal / N;
		Also we can calculate the tensorIdx i = warpIdxGlobal % N;
	Here we need the access guards: I.e. if (x < numVNodes) do_the_matmul(...);
	
	Code works and is fast as fuck! -> Nice! Now we need to fix the random number filling algorithm. We somehow want to multithread that badboy. It could either be done on the GPU or the CPU, but I am thinking that fewer threads are better suited, bc a seed needs to be created for each thread. We should employ OpenMP threads for that, I think. Not sure how one would tell cmake to use OpenMP ... the linking flag was -fopenmp.
	Okay, actually we dont need a proper random number filling system ... just fill in some numbers that are somewhat random. We can do this on the GPU and after that has been implemented, one can finally do the benchmarks on the A100. Christoph had the idea of just creating 1000 random numbers and copy them into the lattice object. The 1000 random numbers are created by the CPU, then uploaded to device memory and finally copied into the lattice container. Okay so I have this array rn[1000] now, how are we gonna do the indexing?
	So I ended up doing all that work on the host using openMP. I am a bit confused, however, how openMP works with cmake. I have to get better with CMake in general.

	This might be a good point to do some clean up. I want to rewrite Lattice<tobj> to not deal with the memory management and therefore write a new class LatticeMemoryManager, that does the memory stuff. This begs the question, however, how the matmul kernel call is managed. LatticeMemoryManager might just need to wrap that. This makes me believe that my idea might not be that great. I think for now I stick with my current concept.

	Oooookaaaayy. This means we can finally advance to stencils. Here it gets complicated. We need geometrical information on the grid. I have to extensively think about this. So, after some consideration here are my findings. First, we need 9 matrix fields rather than one, since we are dealing with gauge fields of course. This is easily solved by simply creating a list of fields. Secondly, the number of vNodes is equal to lenLane. The volume of vNodes is gridvol / lenLane. This is not correctly represented in the code ... I need to rename numVNodes to sizeVNode. DONE! Okay, now we deal with the stencil. It is basically a function that defines which fields are matmul'ed with what offset. As Christoph suggested, it makes sense to start with a simple stencil function that only does so with one matrix field -> lets call it simple_stencil. This function will require to rewrite the matmul completely. I really need to think about how the memory is mapped to the grid. To this end I should ask myself the following questions:
		- Where do I find the lattice element for x.y.z.t? (I should not implement this btw: bc of tensorview.h)
			-> the idx i of tobj * data can be thought of being mapped to the first vNode (i.e. the Node with the smallest x.y.z.t elements). If the vNode has extends of X.Y.Z.T, then (assuming that x.y.z.t are element of the first vNode ofc):
				-> i = x * Y*Z*T + y * Z*T + z * T + t
				-> x = i / Y*Z*T
				-> y = (i - x * Y*Z*T) / (Z*T)  
					or y = (i % Y*Z*T) / (Z*T)
					or MAYBE y = i / (Z*T) - x * Y
				-> z = (i - x * Y*Z*T - y * Z*T) / T
					or z = (i % Z*T) / T
					or MAYBE z = i / T - x * Y*Z - y * Z
				-> t = i - x * Y*Z*T - y * Z*T - z * T
					or t = i % T
		- Where do I find the neighbor to lattice element x.y.z.t in terms of i and l?
			-> l is the index of the vNode (aka the index that walks through a lane) here
			-> assume we have a grid of vNodes with extends vX.vY.vZ.vT, then l maps analogous to question 1.
			-> firstly we need to find the vNode, which should simply be x/X.y/Y.z/Z.t/T where x is the whole grid index and X the size of a vNode in x direction
			-> also, the vNode internal index is x%X.y%Y.z%Z.t%T
			-> next we need to check if the neighbor is in another node or the same.
				- assume it is the same
					-> calculate x.y.z.t of neighbor 
					-> calculate vx.vy.vz.vt and subsequentially i and l -> ezzzzz
		- Is there any weird stuff happening when being at the border of a vNode?
			-> not really, the mapping changes a bit in l and i but thats it
	
	Okay lets talk implementation details. Almost all we just discussed should be implemented in the Grid class. I think upon creation a neighbor array should be instantiated which maps a lattice point (i,l) to its neighbors [(i0,l0), (i1, l1), ...]. Maybe this array is a bit redundant (especially if one has a lot of lattice objects about) but thats alright. Really I should first write a function that does this mapping and then consider if I need such an array.
	Alright. The mapping between flat and cart seems to work just fine. Maybe I should check it with several vNode sizes though. Aight, also checked that. Now let's talk nearest neighbors. I need a function that takes flat or cart coordinates and creates a list of nearest neighbors as flattened coordinates. The functions works for cart argument. Nice! Now for flat arguments. Done! This leads us inevitably to the stencil. Or should we do the neighbor lookup array first? Or maybe it is instantiated with the stencil class? How does the stencil class even look like? Should have a run() function to execute and a constructor to pass the parameters. But I need a GaugeField class, I fear. Its basically an array of references to lattice matrix fields.
	Okay, so the problem with the boundaries is the following: An arrow within a node only changes the n index but not l. Therefore the lineIdxMap is just the identity. For arrows that cross a boundary however, the lineIdxMap is not the identity anymore. We need to find that map before executing the kernel. We also need some kind of border detection function. Sweet, I have a way to extract the thread mapping for different points in the vNode. Only need a fast way to check which points in the vNode lead to boundary crossing. Here, the idea from yesterday comes in. Just calculate the coordinate with the basic rule and check whether it is == 0 (or == V_i - 1). Could be that this process is very slow though. Alright, so I need the thread mapping and check whether a considered point is on a critical border. So I finally need to write the kernel yaaay.
	Kernel written but not tested for correctness. I encountered a weird problem though: On my notebook the GPU is not caching properly. The code works fine on juwels-booster, however. I will do the same check on my home station. It is so awkward, that even the old code, i.e., the matmul kernel, does not make use of the cache anymore. 
	I will check the kernel for correctness now. To this end I need to do some coordinate specific filling of the matrix and vector fields. How about the following:
		1. For Matrices:
			M(x,i,j) = x*1000 + i*N + j;
		2. For Vectors:
			V(x,j) = x*1000/N + j;
		-> resulting in (for x -> x+1)
			V_res(x, i) = sum_j ((x+1)*1000 + i*N + j) (x*1000/N + j) 
	I need setters and getters for the elements on the lattice. Problem: I do not have a TensorView class or something like that. So, do I create one? I think not. Instead just require the tensor indices directly in the getter/setter.
	Okay, wow. We need the CUDA toolkit 12.x for nvcc :) Well, that did not work ... unfortunately. Maybe, I have to sfinae those functions away. Interestingly we do not need to sfinae it away. I thought that we might encounter a compiler error, bc the matrix accessing thingy should not work for vectors, but whatever. Interestingly, the compiler error only occurs, when the get function of a vector lattice is CALLED as if for a matrix field. That is actually kinda nifty ... but using the "requires" method would have been much cleaner.
	Okay, now that we have getters and setters after like one and a half day, we can continue with checking the stencil for correctness. Aaaaaand I checked it ... it happens to be working. Very nice. The next step is matmul with multiple right hand sides. But in what data format are the vector fields present. There are the following options:
		- write a class that contains the data of several vector fields:
			+ could be copied by kernel
			+ would be easy to check if they all have the same underlying grid (because there is only one)
			- a lot of extra code
			- could actually be quite heavy on the stack
		- array of vectorfields
			+ easy to code
			+ could be copied by the kernel
			- redundant grid objects
			- heavy on the stiggidy stack
			- not sure whether the constructor is called ... which would result in a heavy memory leak
		- array of references or pointers -> lets go with this version
			+ easy to code
			+ leightweight on the stack
			+ no unintended call of the constructor
			+ relatively easy to create a respective pointer of pointers for the kernel call
			- a bit dangerous in terms of persistence of objects and such
	Also I might just refactor all the matmul stuff into a new file to clear out the lattice class a little. Done! I went for the array of pointers option. I hope that was a smart move.
	I need to rethink the mapping of the forloop domains to the threads, since I have an additional idx b now. Let's try to not introduce additional threads and instead add another forloop into the kernel
	Okay, so now the batch loop is explicit and performance isn't exactly bad. It is basically similar to normal matmul, where for some parameters it slightly goes beyond that. One big question that I need to answer is the memory layout of the vector batch. Also I need to think about how to do efficient caching of the operants -> look into __shared__ memory.







