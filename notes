This is the notes file for cugrid


TODO:
	- make slides to describe the current situation
		- description of grid and tensor setup -> done
		- screenshot of nsight -> done
		- benchmark times -> done
	- then proceed to rewrite the kernel calls
		- read about and test the grid structure -> not gonna do that
		- if that does not work ... rewrite to for loop over virtual nodes inside of kernel



THOUGHTS:
    How are we gonna test the matmul for correctness? Maybe assist by numpy, or use some mathematical property?
    -> Fill vector with 1s and the rows of the matrices with numbers that we now the sum of and then check if the correct value of the sum is in the resulting vector
    -> Worked very nicely. I am convinced that malmul produces correct results

    Now we need to benchmark the matrix vector multiplication. But how?
    -> Just repeated matmul on several lattice sizes?
    -> Also want to test the effect of lenLane, even though this is not so important. Especially bc atm it needs to be set to the number of threads in one warp.

    Okay, the code seems to work. But how do I calculate the bandwidth and the flops? I have a grid of 8*16*16*32 with 64 dimensions (i.e. matrices: 64x64 and vector: 64) (double prec.) and 50 matmul operations took 2.405 sec. 
    So. How much bandwidth did we employ? To my understanding one matrix - vector multiplication takes N^2 + N memory loads. I.e. a total of (N^2 + N)*sizeof(double) bytes. But do I need to also take into account the write operation into the resulting vector? I do not think so. Okay, so ... with our parameters thats a total of sizeof(double)*(N^2 + 2*N)*8*16*16*32.
    The result is not very nice: something around 50 Gb/s ... meh.
    What about the flops? And how would it perform on an A100? Lets do flops first. So ... we have N^2 additions and multiplications. That makes a total of 2*N^2 flops per lattice site.
    Flops are bad ... like really bad. Out of the 20.31 TFlops, I get 500 MFlops. For 4-byte float that is.

    Okay, now we want to run this on juwels-booster. To this end we need to ssh into one of the computers of the physics CIP pool and set it up there.
    -> I made it into juwels-booster! The trick is to specify the key, that one wants to use, i.e., ssh sizmann1@juwels-booster.fz-juelich.de -i juwelsbooster.
    The next step is to build a source script and get an interactive session for one A100.
    -> got the session with $salloc --account=gm2dwf --gres=gpu:1 --nodes=1 --time=01:00:00 
        and ran the benchmark with srun ./bench_matmul
    -> Managed to do that :) But results were horrific:
        DURATION: 3586898
        BANDWIDTH: 30169 MBytes/sec
        ARITHMETICS: 299.351 Mflops

    I need to use diagnostics to find out what is going on. What about nsight? Let's find out!
	-> got it to work. The executable is /opt/nvidia/nsight-compute/2022.4.1/ncu-ui and needs to be executed with root privileges. The autoprofiling option together with the jump-to-next-kernel-call-button provides details about the kernel call. The program warns that the kernel calls are too small. 
	My suspicion about the kernel call overhead might have been right. In order to check that, we need to rewrite the code a little bit. I am thinking about employing the grid structure of cuda here, since it would require the least amount of rewriting. Without using this, I would need to substanially rethink on how to do the element indexing by thread identifier.

	Here is an important finding: Making the grid super small (2.2.2.4) and the matrix super large (N = 1024) makes the kernel very efficient in terms of memory bandwidth (also blocksize = 256). This means that indeed the number of kernel calls is problematic. Interestingly on the 3070mobile the performance descreases with N = 2048 and larger.

	So ... in order to rewrite everything to a singular kernel call we need to remap the looping domains to the threads. We have blockIdx, threadIdx and subsequentially laneIdx in terms of threads. Also we have latticeIdx, tensorIdcs and laneIdx. TensorIdcs become warp indices etc ... but how? Lets assume a given blocksize(bs) and therefore a given number of lanes per block (#lpb)
	We have x -> numVNodes, i(,j) -> N, l -> lenLane. This means we have numVNodes*N "lane calls", ergo numVNodes*N/#lpb blocks. What do we do if the division does not work out? -> add access guards! The kernel call becomes:
		blocks : (numVNodes*N + #lpb - 1)/#lpb
		threads: bs
		ker_matmul<<<blocks, threads>>>(res, mfield, vfield);
	The matmul function in iMatrix needs to change at well, or maybe we pull all the code into the Lattice class. Lets assume we do the second: 
		Then we first calculate the global warpIdx by warpIdxGlobal = blockIdx.x * #lpb + warpIdx. 
		From there we can calculate the vNodeIdx(i.e. x) x = warpIdxGlobal / N;
		Also we can calculate the tensorIdx i = warpIdxGlobal % N;
	Here we need the access guards: I.e. if (x < numVNodes) do_the_matmul(...);




