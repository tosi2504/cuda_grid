This is the notes file for cugrid


TODO:
	- make slides to describe the current situation
		- description of grid and tensor setup -> done
		- screenshot of nsight -> done
		- benchmark times -> done
	- then proceed to rewrite the kernel calls
		- read about and test the grid structure -> not gonna do that
		- if that does not work ... rewrite to for loop over virtual nodes inside of kernel -> DONE!
	- finish the presentation
		- benchmarks on A100
	- rework random number filling
		- work on CPU




THOUGHTS:
    How are we gonna test the matmul for correctness? Maybe assist by numpy, or use some mathematical property?
    -> Fill vector with 1s and the rows of the matrices with numbers that we now the sum of and then check if the correct value of the sum is in the resulting vector
    -> Worked very nicely. I am convinced that malmul produces correct results

    Now we need to benchmark the matrix vector multiplication. But how?
    -> Just repeated matmul on several lattice sizes?
    -> Also want to test the effect of lenLane, even though this is not so important. Especially bc atm it needs to be set to the number of threads in one warp.

    Okay, the code seems to work. But how do I calculate the bandwidth and the flops? I have a grid of 8*16*16*32 with 64 dimensions (i.e. matrices: 64x64 and vector: 64) (double prec.) and 50 matmul operations took 2.405 sec. 
    So. How much bandwidth did we employ? To my understanding one matrix - vector multiplication takes N^2 + N memory loads. I.e. a total of (N^2 + N)*sizeof(double) bytes. But do I need to also take into account the write operation into the resulting vector? I do not think so. Okay, so ... with our parameters thats a total of sizeof(double)*(N^2 + 2*N)*8*16*16*32.
    The result is not very nice: something around 50 Gb/s ... meh.
    What about the flops? And how would it perform on an A100? Lets do flops first. So ... we have N^2 additions and multiplications. That makes a total of 2*N^2 flops per lattice site.
    Flops are bad ... like really bad. Out of the 20.31 TFlops, I get 500 MFlops. For 4-byte float that is.

    Okay, now we want to run this on juwels-booster. To this end we need to ssh into one of the computers of the physics CIP pool and set it up there.
    -> I made it into juwels-booster! The trick is to specify the key, that one wants to use, i.e., ssh sizmann1@juwels-booster.fz-juelich.de -i juwelsbooster.
    The next step is to build a source script and get an interactive session for one A100.
    -> got the session with $salloc --account=gm2dwf --gres=gpu:1 --nodes=1 --time=01:00:00 
        and ran the benchmark with srun ./bench_matmul
    -> Managed to do that :) But results were horrific:
        DURATION: 3586898
        BANDWIDTH: 30169 MBytes/sec
        ARITHMETICS: 299.351 Mflops

    I need to use diagnostics to find out what is going on. What about nsight? Let's find out!
	-> got it to work. The executable is /opt/nvidia/nsight-compute/2022.4.1/ncu-ui and needs to be executed with root privileges. The autoprofiling option together with the jump-to-next-kernel-call-button provides details about the kernel call. The program warns that the kernel calls are too small. 
	My suspicion about the kernel call overhead might have been right. In order to check that, we need to rewrite the code a little bit. I am thinking about employing the grid structure of cuda here, since it would require the least amount of rewriting. Without using this, I would need to substanially rethink on how to do the element indexing by thread identifier.

	Here is an important finding: Making the grid super small (2.2.2.4) and the matrix super large (N = 1024) makes the kernel very efficient in terms of memory bandwidth (also blocksize = 256). This means that indeed the number of kernel calls is problematic. Interestingly on the 3070mobile the performance descreases with N = 2048 and larger.

	So ... in order to rewrite everything to a singular kernel call we need to remap the looping domains to the threads. We have blockIdx, threadIdx and subsequentially laneIdx in terms of threads. Also we have latticeIdx, tensorIdcs and laneIdx. TensorIdcs become warp indices etc ... but how? Lets assume a given blocksize(bs) and therefore a given number of lanes per block (#lpb)
	We have x -> numVNodes, i(,j) -> N, l -> lenLane. This means we have numVNodes*N "lane calls", ergo numVNodes*N/#lpb blocks. What do we do if the division does not work out? -> add access guards! The kernel call becomes:
		blocks : (numVNodes*N + #lpb - 1)/#lpb
		threads: bs
		ker_matmul<<<blocks, threads>>>(res, mfield, vfield);
	The matmul function in iMatrix needs to change at well, or maybe we pull all the code into the Lattice class. Lets assume we do the second: 
		Then we first calculate the global warpIdx by warpIdxGlobal = blockIdx.x * #lpb + warpIdx. 
		From there we can calculate the vNodeIdx(i.e. x) x = warpIdxGlobal / N;
		Also we can calculate the tensorIdx i = warpIdxGlobal % N;
	Here we need the access guards: I.e. if (x < numVNodes) do_the_matmul(...);
	
	Code works and is fast as fuck! -> Nice! Now we need to fix the random number filling algorithm. We somehow want to multithread that badboy. It could either be done on the GPU or the CPU, but I am thinking that fewer threads are better suited, bc a seed needs to be created for each thread. We should employ OpenMP threads for that, I think. Not sure how one would tell cmake to use OpenMP ... the linking flag was -fopenmp.
	Okay, actually we dont need a proper random number filling system ... just fill in some numbers that are somewhat random. We can do this on the GPU and after that has been implemented, one can finally do the benchmarks on the A100. Christoph had the idea of just creating 1000 random numbers and copy them into the lattice object. The 1000 random numbers are created by the CPU, then uploaded to device memory and finally copied into the lattice container. Okay so I have this array rn[1000] now, how are we gonna do the indexing?
	So I ended up doing all that work on the host using openMP. I am a bit confused, however, how openMP works with cmake. I have to get better with CMake in general.

	This might be a good point to do some clean up. I want to rewrite Lattice<tobj> to not deal with the memory management and therefore write a new class LatticeMemoryManager, that does the memory stuff. This begs the question, however, how the matmul kernel call is managed. LatticeMemoryManager might just need to wrap that. This makes me believe that my idea might not be that great. I think for now I stick with my current concept.

	Oooookaaaayy. This means we can finally advance to stencils. Here it gets complicated. We need geometrical information on the grid. I have to extensively think about this. So, after some consideration here are my findings. First, we need 9 matrix fields rather than one, since we are dealing with gauge fields of course. This is easily solved by simply creating a list of fields. Secondly, the number of vNodes is equal to lenLane. The volume of vNodes is gridvol / lenLane. This is not correctly represented in the code ... I need to rename numVNodes to sizeVNode. DONE! Okay, now we deal with the stencil. It is basically a function that defines which fields are matmul'ed with what offset. As Christoph suggested, it makes sense to start with a simple stencil function that only does so with one matrix field -> lets call it simple_stencil. This function will require to rewrite the matmul completely. I really need to think about how the memory is mapped to the grid. To this end I should ask myself the following questions:
		- Where do I find the lattice element for x.y.z.t? (I should not implement this btw: bc of tensorview.h)
			-> the idx i of tobj * data can be thought of being mapped to the first vNode (i.e. the Node with the smallest x.y.z.t elements). If the vNode has extends of X.Y.Z.T, then (assuming that x.y.z.t are element of the first vNode ofc):
				-> i = x * Y*Z*T + y * Z*T + z * T + t
				-> x = i / Y*Z*T
				-> y = (i - x * Y*Z*T) / (Z*T)  
					or y = (i % Y*Z*T) / (Z*T)
					or MAYBE y = i / (Z*T) - x * Y
				-> z = (i - x * Y*Z*T - y * Z*T) / T
					or z = (i % Z*T) / T
					or MAYBE z = i / T - x * Y*Z - y * Z
				-> t = i - x * Y*Z*T - y * Z*T - z * T
					or t = i % T
		- Where do I find the neighbor to lattice element x.y.z.t?
		- Is there any weird stuff happening when being at the border of a vNode?




