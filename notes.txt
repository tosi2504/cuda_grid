TODO:

THOUGHTS:
    How are we gonna test the matmul for correctness? Maybe assist by numpy, or use some mathematical property?
    -> Fill vector with 1s and the rows of the matrices with numbers that we now the sum of and then check if the correct value of the sum is in the resulting vector
    -> Worked very nicely. I am convinced that malmul produces correct results

    Now we need to benchmark the matrix vector multiplication. But how?
    -> Just repeated matmul on several lattice sizes?
    -> Also want to test the effect of lenLane, even though this is not so important. Especially bc atm it needs to be set to the number of threads in one warp.

    Okay, the code seems to work. But how do I calculate the bandwidth and the flops? I have a grid of 8*16*16*32 with 64 dimensions (i.e. matrices: 64x64 and vector: 64) (double prec.) and 50 matmul operations took 2.405 sec. 
    So. How much bandwidth did we employ? To my understanding one matrix - vector multiplication takes N^2 + N memory loads. I.e. a total of (N^2 + N)*sizeof(double) bytes. But do I need to also take into account the write operation into the resulting vector? I do not think so. Okay, so ... with our parameters thats a total of sizeof(double)*(N^2 + 2*N)*8*16*16*32.
    The result is not very nice: something around 50 Gb/s ... meh.
    What about the flops? And how would it perform on an A100? Lets do flops first. So ... we have N^2 additions and multiplications. That makes a total of 2*N^2 flops per lattice site.
    Flops are bad ... like really bad. Out of the 20.31 TFlops, I get 500 MFlops. For 4-byte float that is.

    Okay, now we want to run this on juwels-booster. To this end we need to ssh into one of the computers of the physics CIP pool and set it up there.
    -> I made it into juwels-booster! The trick is to specify the key, that one wants to use, i.e., ssh sizmann1@juwels-booster.fz-juelich.de -i juwelsbooster.
    The next step is to build a source script and get an interactive session for one A100.
    -> got the session with $salloc --account=gm2dwf --gres=gpu:1 --nodes=1 --time=01:00:00 
        and ran the benchmark with srun ./bench_matmul
    -> Managed to do that :) But results were horrific:
        DURATION: 3586898
        BANDWIDTH: 30169 MBytes/sec
        ARITHMETICS: 299.351 Mflops

    I need to use diagnostics to find out what is going on. What about nsight? Let's find out!
	-> got it to work. The executable is /opt/nvidia/nsight-compute/2022.4.1/ncu-ui and needs to be executed with root privileges. The autoprofiling option together with the jump-to-next-kernel-call-button provides details about the kernel call. The program warns that the kernel calls are too small. 
	My suspicion about the kernel call overhead might have been right. In order to check that, we need to rewrite the code a little bit. I am thinking about employing the grid structure of cuda here, since it would require the least amount of rewriting. Without using this, I would need to substanially rethink on how to do the element indexing by thread identifier.

	Here is an important finding: Making the grid super small (2.2.2.4) and the matrix super large (N = 1024) makes the kernel very efficient in terms of memory bandwidth (also blocksize = 256). This means that indeed the number of kernel calls is problematic. Interestingly on the 3070mobile the performance descreases with N = 2048 and larger.

	So ... in order to rewrite everything to a singular kernel call we need to remap the looping domains to the threads. We have blockIdx, threadIdx and subsequentially laneIdx in terms of threads. Also we have latticeIdx, tensorIdcs and laneIdx. TensorIdcs become warp indices etc ... but how? Lets assume a given blocksize(bs) and therefore a given number of lanes per block (#lpb)
	We have x -> numVNodes, i(,j) -> N, l -> lenLane. This means we have numVNodes*N "lane calls", ergo numVNodes*N/#lpb blocks. What do we do if the division does not work out? -> add access guards! The kernel call becomes:
		blocks : (numVNodes*N + #lpb - 1)/#lpb
		threads: bs
		ker_matmul<<<blocks, threads>>>(res, mfield, vfield);
	The matmul function in iMatrix needs to change at well, or maybe we pull all the code into the Lattice class. Lets assume we do the second: 
		Then we first calculate the global warpIdx by warpIdxGlobal = blockIdx.x * #lpb + warpIdx. 
		From there we can calculate the vNodeIdx(i.e. x) x = warpIdxGlobal / N;
		Also we can calculate the tensorIdx i = warpIdxGlobal % N;
	Here we need the access guards: I.e. if (x < numVNodes) do_the_matmul(...);
	
	Code works and is fast as fuck! -> Nice! Now we need to fix the random number filling algorithm. We somehow want to multithread that badboy. It could either be done on the GPU or the CPU, but I am thinking that fewer threads are better suited, bc a seed needs to be created for each thread. We should employ OpenMP threads for that, I think. Not sure how one would tell cmake to use OpenMP ... the linking flag was -fopenmp.
	Okay, actually we dont need a proper random number filling system ... just fill in some numbers that are somewhat random. We can do this on the GPU and after that has been implemented, one can finally do the benchmarks on the A100. Christoph had the idea of just creating 1000 random numbers and copy them into the lattice object. The 1000 random numbers are created by the CPU, then uploaded to device memory and finally copied into the lattice container. Okay so I have this array rn[1000] now, how are we gonna do the indexing?
	So I ended up doing all that work on the host using openMP. I am a bit confused, however, how openMP works with cmake. I have to get better with CMake in general.

	This might be a good point to do some clean up. I want to rewrite Lattice<tobj> to not deal with the memory management and therefore write a new class LatticeMemoryManager, that does the memory stuff. This begs the question, however, how the matmul kernel call is managed. LatticeMemoryManager might just need to wrap that. This makes me believe that my idea might not be that great. I think for now I stick with my current concept.

	Oooookaaaayy. This means we can finally advance to stencils. Here it gets complicated. We need geometrical information on the grid. I have to extensively think about this. So, after some consideration here are my findings. First, we need 9 matrix fields rather than one, since we are dealing with gauge fields of course. This is easily solved by simply creating a list of fields. Secondly, the number of vNodes is equal to lenLane. The volume of vNodes is gridvol / lenLane. This is not correctly represented in the code ... I need to rename numVNodes to sizeVNode. DONE! Okay, now we deal with the stencil. It is basically a function that defines which fields are matmul'ed with what offset. As Christoph suggested, it makes sense to start with a simple stencil function that only does so with one matrix field -> lets call it simple_stencil. This function will require to rewrite the matmul completely. I really need to think about how the memory is mapped to the grid. To this end I should ask myself the following questions:
		- Where do I find the lattice element for x.y.z.t? (I should not implement this btw: bc of tensorview.h)
			-> the idx i of tobj * data can be thought of being mapped to the first vNode (i.e. the Node with the smallest x.y.z.t elements). If the vNode has extends of X.Y.Z.T, then (assuming that x.y.z.t are element of the first vNode ofc):
				-> i = x * Y*Z*T + y * Z*T + z * T + t
				-> x = i / Y*Z*T
				-> y = (i - x * Y*Z*T) / (Z*T)  
					or y = (i % Y*Z*T) / (Z*T)
					or MAYBE y = i / (Z*T) - x * Y
				-> z = (i - x * Y*Z*T - y * Z*T) / T
					or z = (i % Z*T) / T
					or MAYBE z = i / T - x * Y*Z - y * Z
				-> t = i - x * Y*Z*T - y * Z*T - z * T
					or t = i % T
		- Where do I find the neighbor to lattice element x.y.z.t in terms of i and l?
			-> l is the index of the vNode (aka the index that walks through a lane) here
			-> assume we have a grid of vNodes with extends vX.vY.vZ.vT, then l maps analogous to question 1.
			-> firstly we need to find the vNode, which should simply be x/X.y/Y.z/Z.t/T where x is the whole grid index and X the size of a vNode in x direction
			-> also, the vNode internal index is x%X.y%Y.z%Z.t%T
			-> next we need to check if the neighbor is in another node or the same.
				- assume it is the same
					-> calculate x.y.z.t of neighbor 
					-> calculate vx.vy.vz.vt and subsequentially i and l -> ezzzzz
		- Is there any weird stuff happening when being at the border of a vNode?
			-> not really, the mapping changes a bit in l and i but thats it
	
	Okay lets talk implementation details. Almost all we just discussed should be implemented in the Grid class. I think upon creation a neighbor array should be instantiated which maps a lattice point (i,l) to its neighbors [(i0,l0), (i1, l1), ...]. Maybe this array is a bit redundant (especially if one has a lot of lattice objects about) but thats alright. Really I should first write a function that does this mapping and then consider if I need such an array.
	Alright. The mapping between flat and cart seems to work just fine. Maybe I should check it with several vNode sizes though. Aight, also checked that. Now let's talk nearest neighbors. I need a function that takes flat or cart coordinates and creates a list of nearest neighbors as flattened coordinates. The functions works for cart argument. Nice! Now for flat arguments. Done! This leads us inevitably to the stencil. Or should we do the neighbor lookup array first? Or maybe it is instantiated with the stencil class? How does the stencil class even look like? Should have a run() function to execute and a constructor to pass the parameters. But I need a GaugeField class, I fear. Its basically an array of references to lattice matrix fields.
	Okay, so the problem with the boundaries is the following: An arrow within a node only changes the n index but not l. Therefore the lineIdxMap is just the identity. For arrows that cross a boundary however, the lineIdxMap is not the identity anymore. We need to find that map before executing the kernel. We also need some kind of border detection function. Sweet, I have a way to extract the thread mapping for different points in the vNode. Only need a fast way to check which points in the vNode lead to boundary crossing. Here, the idea from yesterday comes in. Just calculate the coordinate with the basic rule and check whether it is == 0 (or == V_i - 1). Could be that this process is very slow though. Alright, so I need the thread mapping and check whether a considered point is on a critical border. So I finally need to write the kernel yaaay.
	Kernel written but not tested for correctness. I encountered a weird problem though: On my notebook the GPU is not caching properly. The code works fine on juwels-booster, however. I will do the same check on my home station. It is so awkward, that even the old code, i.e., the matmul kernel, does not make use of the cache anymore. 
	I will check the kernel for correctness now. To this end I need to do some coordinate specific filling of the matrix and vector fields. How about the following:
		1. For Matrices:
			M(x,i,j) = x*1000 + i*N + j;
		2. For Vectors:
			V(x,j) = x*1000/N + j;
		-> resulting in (for x -> x+1)
			V_res(x, i) = sum_j ((x+1)*1000 + i*N + j) (x*1000/N + j) 
	I need setters and getters for the elements on the lattice. Problem: I do not have a TensorView class or something like that. So, do I create one? I think not. Instead just require the tensor indices directly in the getter/setter.
	Okay, wow. We need the CUDA toolkit 12.x for nvcc :) Well, that did not work ... unfortunately. Maybe, I have to sfinae those functions away. Interestingly we do not need to sfinae it away. I thought that we might encounter a compiler error, bc the matrix accessing thingy should not work for vectors, but whatever. Interestingly, the compiler error only occurs, when the get function of a vector lattice is CALLED as if for a matrix field. That is actually kinda nifty ... but using the "requires" method would have been much cleaner.
	Okay, now that we have getters and setters after like one and a half day, we can continue with checking the stencil for correctness. Aaaaaand I checked it ... it happens to be working. Very nice. The next step is matmul with multiple right hand sides. But in what data format are the vector fields present. There are the following options:
		- write a class that contains the data of several vector fields:
			+ could be copied by kernel
			+ would be easy to check if they all have the same underlying grid (because there is only one)
			- a lot of extra code
			- could actually be quite heavy on the stack
		- array of vectorfields
			+ easy to code
			+ could be copied by the kernel
			- redundant grid objects
			- heavy on the stiggidy stack
			- not sure whether the constructor is called ... which would result in a heavy memory leak
		- array of references or pointers -> lets go with this version
			+ easy to code
			+ leightweight on the stack
			+ no unintended call of the constructor
			+ relatively easy to create a respective pointer of pointers for the kernel call
			- a bit dangerous in terms of persistence of objects and such
	Also I might just refactor all the matmul stuff into a new file to clear out the lattice class a little. Done! I went for the array of pointers option. I hope that was a smart move.
	I need to rethink the mapping of the forloop domains to the threads, since I have an additional idx b now. Let's try to not introduce additional threads and instead add another forloop into the kernel
	Okay, so now the batch loop is explicit and performance isn't exactly bad. It is basically similar to normal matmul, where for some parameters it slightly goes beyond that. One big question that I need to answer is the memory layout of the vector batch. Also I need to think about how to do efficient caching of the operants -> look into __shared__ memory.
	Christoph said I should distribute the batch index over threads, lets see how that could be done. Also I need to extensively benchmark my code on the A100s. Let's continue with the batch index. Before I had i and n mapped to the warpIdxGlobal. Now I simply need to add b. This means that now:
		- n = w.warpIdxGlobal / (N*batchsize);
		- i = (w.warpIdxGlobal % (N*batchsize)) / batchsize;
		- b = w.warpIdxGlobal % batchsize;
	I implemented this, it was very easy. It seems to have a significant increase in bandwidth on my notebook. Lets see the numbers on juwels booster. Alright the numbers are (in TB/s):
	- double, lenLane 32
		- grid 16.16.16.16, N 64, batch 10: 
			- naive: 1.20
			- improved: 2.03
		- grid 16.32.32.32, N 64, batch 10:
			- naive: 1.25
			- improved: 2.19
		- grid 16.32.32.32, N 32, batch 20:
			- naive: 1.28
			- improved: 2.27 (1.41 x 1.6!!!)
	With these performance stats, the code is better than the "provable optimum" of matrix vector multiplication. But I am pretty sure than playing around with cacheblocking could lead to even more performance gains. This is not entirely trivial, however, since the outer tensor index i is not part of the for loop but instead mapped by the warpIdxGlobal, as is b. Therefore, I have no control over the order, in which it is executed. I should also play around with the blocksize. This could actually be huge, now that I think about it. The blocksize ultimatively determines, how much data is loaded into cache ... or does it??? The L2 cache is non-local to one SM. But I cannot enforce block synchronisation sadly. With smart thread to loopdomain mapping, cache blocking may come even for free. So, that sparks the immediate question, how the mapping would look like for blocksizes delta_i and delta_b, where we require that a block of size delta_i and delta_b are entirely dealt with by one threadblock. Threadblocks are rather small though (max 32 = 4*4*2 warps). Maybe using blocks with delta_i = 4 and delta_b = 4 (16 warps, 512 threads) could be advantageous. If cacheblocking is done within a block, can we guarantee that it is PROPERLY done across blocks. I.e. does it factorize? Huh. Also when doing cache blocking within a threadblock, the shared memory becomes very interesting again.
	Okay we got a small performance boost out of thread blocking. But I think it could be interesting to research that further. In total honesty, I expected more of a speedup. Nsight-compute might give some insight into this.
	Actually, nvm. Bringing the batch index over to the thread domain was a great improvement. I will do the same for the batch-simplestencil and see what happens there.


	Alright, I am a bit lost here. Not sure where to continue now. I have some implementation for batched matmul which seems to offer some performance boost. Christoph wants me to do three things:
		- use cuBLAS with my library
		- use cuBLAS without interleaved memory
		- use tensorcores directly (for TF32 and FP16)
	This means that I have to build expertise in these fields. Alright, let us download cublas and see what happens.

	Okay, I have learned a lot. Apparently, cuBLAS uses column major and only allows the fundamental data types, which makes sense. So, in order to use it, two major changes would be needed to be done on the layout. First, one needs to remove the interleaving of tensor and node idices. Secondly, the tensors need to be stored in column major. I could just implement this right away and see whats faster ... my implementation or the cuBLAS version. Whats a little bit concerning, however, is that cuBLAS does not offer multiple rhs routines. This could be fixed, however, by first aligning the vectors as matrix.
	What do I do exactly, now? -> cugrid2
	cugrid2 should work fundamentally similar to cugrid from an API aspect. I need a tensor class, a grid class and a lattice class. The grid class is not important for now. I also want some kind of translation between the two datastorage backends. So I thought about it even more. I think it is very reasonable to write a cugrid version using cuBLAS. It should not take very long and be easily debuggable. Then I will write a full-fleched stencil kernel (finally uargh!). The cublas-version of cugrid should be integrated seamlessly.
	Should I move over to MESON? Could be worth it. Let's see what the have for header-only libraries. I will play around with meson for a bit and build some expertise.
	Moved to meson successfully. Implemented basic bTensor functionalities. Gonna implement batched matmul using cuBLAS and a simple lattice container for GPU-MEM management. Lattice container done. Now the matmul.
	There are several cuBLAS versions of interest:
		- cuBLAS (normal)	
			- Apparently the input datalayout of the matrix can be set by a cublasOperation_t argument (here: CUBLAS_OP_T)
		- cuBLASXt -> manages several GPUs, no devc mem management required (only type 3 routines)
		- cuBLASLt -> allows more types of mem layouts, only matrix - matrix
	Let's start with normal cuBLAS, i.e., batched matrix-vector. I have implemented it AND moved to cuda 12.3 in the process -> nice! But what next? Lets debug and benchmark it! cublas calls are asynchronous -> in order to benchmark one needs to use cudaDeviceSynchronize(). The benchmarks look very good -> always over 90% memory bandwidth achieved on my notebook GPU. What is the next step? Think about stencils maybe? Or implement matmul for double and complex numbers :) Double and complex works perfect :) I am starting to like cuBLAS. This means that we should proceed straight to multiple right hand sides. Here we need to check three options: 
	- Just putting the cublas gemv-calls in a for loop -> naive version. 
		- yields performance close to bandwidth but not better
	- Write the matmul kernel by myself -> allow for intelligent caching of data.
		- Okay, this could be funny. Lets completely cache the matrix and lets see what happens. Interestingly complex numbers really suffer in performance (in terms of bandwidth)
		- decent performance, but complex numbers suffer -> maybe find explicit bottleneck? (ask christoph)
	- Reordering the data layout of the vectors to allow for a gemm call (maybe do this in a stream).
		- lets just start with a naive implementation
		- we get decent performance now, but somehow the cublas-gemm call is really slow for doubles and complex doubles. Something is not right here. Turns out that this is specific for my chip. On the A100 it works smoothly.
		- with cudaMalloc for the temporary fields with every matmul_mrhs call the performance degrades slightly. Not sure whether this is a truely limiting factor. For now we will just include it in the benchmarks.

SUMMARY SO FAR: Matrix caching on the shared memory should theoretically be the fastest, but I would need to hand-optimize it and I do not know how (maybe follow siboehm's website??). The copy-and-gemm method outperforms my code by several factors at the moment (at least for larger numRHS). This could change for stencils however. Maybe I just proceed to implement the stencil call and if I feel motivated I also try my own version. I STILL HAVE NOT FOUND THE REASON WHY MY CODE SUCKS SO BADLY FOR COMPLEX NUMBERS AHHHHHHHH. Tried the kernels implemented according to siboehm with complex numbers and they fail too. I have absolutely no clue whats the problem here. Though on should be careful ... one complex multiplication contains 6 FLOPs. Alright, so how should I continue here? cuBlas really is very good ... no arguing. I will proceed with stencils written in cublas ... this should also nicely transfer to how Christoph wants to call these stencils. I think I will make this the objective for the week, i.e. implement, test and bench this.

Okay, so lets talk stencils. I will implement one-direction (mu-)stencils. The general case can then be constructed from those mu-stencils. But how to deal with the borders? -> make two cublas calls: One for the bulk and one for the corresponding border. Without further ado -> let's go! Okay, in more than 2 dimensions the borders are anything but trivial. Let's think about that the next time. There is a gemmBatched cublas routine that could help. Instead of a contiguous batch of elements one can pass a pointer to pointers. That should work nicely.


MEETING WITH CHRISTOPH: Interesting parameter range: N = 32, 64, 128; mrhs = 1, 12, 24, 36, 48, 60; grid = 4.4.4.4, 4.4.8.8, 8.8.8.8, 16.16.16.16

Okay, so how do I perform the benchmarks? Do I write a python script? Could be a good idea. The main-function takes in the benchmark parameters as args and writes the results into a txt-file. I should write one such main for each stencil kernel.

All the data generation is in place. How do I present the data? I want to plot it.
But how?
I need to get data from juwelsbooster to test my analyzing. Data is on the way :)
Turns out the python script was a bad fucking idea and stole 2 weeks of my highly valuable lifetime.
The nested-templatefunction approach together with a stopwatch turned out to be much better.
Now I just need a python script to evaluate this data.
But that shouldnt be too hard.
